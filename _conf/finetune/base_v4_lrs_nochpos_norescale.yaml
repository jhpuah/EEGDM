rng_seeding:
  seed: ???
  workers: True

caching:
  do_cache: False
  root: "./.cached_data/"

trainer:
  _target_: lightning.pytorch.Trainer
  fast_dev_run: False
  accelerator: 'gpu'
  strategy: 'ddp'
  devices: [4, 5, 6, 7]
  logger: 
    - _target_: lightning.pytorch.loggers.wandb.WandbLogger
      name: "classifier_base_v4_lrs_nochpos_norescale"
      project: "proper_finetuning"
      save_dir: "./log/"
  default_root_dir: "./checkpoint"
  callbacks: 
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: "val/kappa"
      mode: "max"
      save_top_k: 1
      every_n_epochs: 1
      dirpath: "./checkpoint/finetune/base_v4_lrs_nochpos_norescale/"
      filename: "classifier"
      save_last: True
    - _target_: src.util.EarlyStoppingWithWarmup
      monitor: "val/kappa"
      mode: "max"
      patience: 3
      warmup: 20
  max_epochs: 50
  min_epochs: 5
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: null
  enable_checkpointing: True
  enable_progress_bar: True
  enable_model_summary: True
  accumulate_grad_batches: 4
  gradient_clip_val: 3
  gradient_clip_algorithm: "norm"
  deterministic: True
  inference_mode: True
  use_distributed_sampler: True
  profiler: null
  detect_anomaly: False
  barebones: False






model:
  diffusion_model_checkpoint: "./checkpoint/pretrain/backbone.ckpt"
  model_kwargs:
    start: 0
    end: null
    diffusion_t: 1
    query: ["gate"] # inter gate feature
    reduce: ["std"] # mean std
    rescale: False
    L: 1000
    window_size: 200
    window_step: 200
    pool_merge: "share" # mix cat share
    multi_query_merge: "seq" # cat seq ind
    d_embed: null
    num_heads: 8
    ff: 4
    stack_struct: "scf"
    n_clst: 4
    final_stack_struct: null
    n_class: 6
    final_act: "pool" # cat pool cls
    dropout: 0.05
    have_ch_pos_embed: False
    final_stack_pos_embed_dim: "TP" # TPN

  ema_kwargs:
    beta: 0.999
    update_after_step: 100
    update_every: 10

  opt_kwargs:
    lr: 1e-5
    weight_decay: 0.05
    betas: [0.9, 0.98]

  sch_kwargs:
    pct_start: 0.1
    max_lr: 5e-4

  criterion_kwargs:
    weight: null  
    reduction: "mean"
    label_smoothing: 0.1
    gamma: 0

  fwd_with_noise: False
  # data_is_cached: False
  run_test_together: False
  cls_version: 4
  lrd_kwargs:
    use_new_setup: True
    no_wd: ["cls_token"]
    bias_1dim_no_wd: True
    # lr_decay: 
    #   - 0.75 
    #   - ["final_stack.layers.2", "final_stack.layers.3"] # 1
    #   - ["final_stack.layers.4", "final_stack.layers.5"] # 2
    #   - ["final_stack.layers.6", "final_stack.layers.7"] # 3

data:
  _target_: builtins.dict
  root: "./data/faithful"
  train_dir: "train"
  val_dir: "val"
  test_dir: "test"
  batch_size: 32
  num_workers: 4
  schema:
    - _target_: dataloader.TUEVDataset.TUEVDataField
      _args_:
        - "signal"
        - _target_: src.util.dynamic_load
          item: torch.float
        - _target_: src.util.dynamic_load
          item: src.util.staged_mu_law
    - _target_: dataloader.TUEVDataset.TUEVDataField
      _args_:
        - "label"
        - _target_: src.util.dynamic_load
          item: torch.long
        - _target_: src.util.dynamic_load
          item: src.util.minus_one
  stft_kwargs: null
